{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf013fb2-8d49-4ad2-80d4-24e91d5e978f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - torch==2.1.2\n",
      "  - adjusttext==0.8\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - https://conda.anaconda.org/conda-forge/win-64\n",
      "  - https://conda.anaconda.org/conda-forge/noarch\n",
      "  - https://repo.anaconda.com/pkgs/main/win-64\n",
      "  - https://repo.anaconda.com/pkgs/main/noarch\n",
      "  - https://repo.anaconda.com/pkgs/r/win-64\n",
      "  - https://repo.anaconda.com/pkgs/r/noarch\n",
      "  - https://repo.anaconda.com/pkgs/msys2/win-64\n",
      "  - https://repo.anaconda.com/pkgs/msys2/noarch\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solving environment: ...working... unsuccessful initial attempt using frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... unsuccessful initial attempt using frozen solve. Retrying with flexible solve.\n"
     ]
    }
   ],
   "source": [
    "%conda install --file requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67fb25c7-0f1f-4491-b16f-3513b1aea35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece==0.2.0 in c:\\users\\fkhel\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentencepiece==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14c3a960-86c4-4e1f-a9fa-65d81ec0dfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting adjustText==0.8\n",
      "  Obtaining dependency information for adjustText==0.8 from https://files.pythonhosted.org/packages/44/36/ac4a2dfefe115be67403bb8c622edf5b6adacbb3afd2ac921af37138ec1c/adjustText-0.8-py3-none-any.whl.metadata\n",
      "  Downloading adjustText-0.8-py3-none-any.whl.metadata (402 bytes)\n",
      "Requirement already satisfied: numpy in c:\\users\\fkhel\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages (from adjustText==0.8) (1.24.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\fkhel\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages (from adjustText==0.8) (3.9.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\fkhel\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages (from matplotlib->adjustText==0.8) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\fkhel\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages (from matplotlib->adjustText==0.8) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\fkhel\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages (from matplotlib->adjustText==0.8) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\fkhel\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages (from matplotlib->adjustText==0.8) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\fkhel\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages (from matplotlib->adjustText==0.8) (23.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\fkhel\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages (from matplotlib->adjustText==0.8) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\fkhel\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages (from matplotlib->adjustText==0.8) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\fkhel\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages (from matplotlib->adjustText==0.8) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\fkhel\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->adjustText==0.8) (1.16.0)\n",
      "Downloading adjustText-0.8-py3-none-any.whl (9.1 kB)\n",
      "Installing collected packages: adjustText\n",
      "Successfully installed adjustText-0.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install adjustText==0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6049c869-5f7a-4dd3-8160-dbfb0fdca912",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27377461-2c9b-4880-9107-940472e66ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm==4.64.1\n",
      "  Obtaining dependency information for tqdm==4.64.1 from https://files.pythonhosted.org/packages/47/bb/849011636c4da2e44f1253cd927cfb20ada4374d8b3a4e425416e84900cc/tqdm-4.64.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.3 kB ? eta -:--:--\n",
      "     ------- -------------------------------- 10.2/57.3 kB ? eta -:--:--\n",
      "     ------- -------------------------------- 10.2/57.3 kB ? eta -:--:--\n",
      "     ------- -------------------------------- 10.2/57.3 kB ? eta -:--:--\n",
      "     ------- -------------------------------- 10.2/57.3 kB ? eta -:--:--\n",
      "     ------- -------------------------------- 10.2/57.3 kB ? eta -:--:--\n",
      "     --------------------------------- ---- 51.2/57.3 kB 154.0 kB/s eta 0:00:01\n",
      "     --------------------------------- ---- 51.2/57.3 kB 154.0 kB/s eta 0:00:01\n",
      "     -------------------------------------- 57.3/57.3 kB 136.9 kB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\fkhel\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages (from tqdm==4.64.1) (0.4.6)\n",
      "Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.1\n",
      "    Uninstalling tqdm-4.66.1:\n",
      "      Successfully uninstalled tqdm-4.66.1\n",
      "Successfully installed tqdm-4.64.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script tqdm.exe is installed in 'C:\\Users\\fkhel\\miniconda3\\envs\\pytorch-gpu-python-3-10\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm==4.64.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00fad9cd-5d0e-4841-bf4e-8cbd51e492b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fkhel\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\fkhel\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "C:\\Users\\fkhel\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    GPT2LMHeadModel, \n",
    "    GPT2Tokenizer, \n",
    "    RobertaForMaskedLM, \n",
    "    RobertaTokenizer, \n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a22368d4-d810-444e-bd26-8225f5cd08b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: unknown command \"instal\" - maybe you meant \"install\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip instal openai==0.28.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34d3aa29-bfe0-48be-bd1e-65a9d2677dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04b14397-a191-440d-882b-10e1d732a93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-decouple\n",
      "  Obtaining dependency information for python-decouple from https://files.pythonhosted.org/packages/a2/d4/9193206c4563ec771faf2ccf54815ca7918529fe81f6adb22ee6d0e06622/python_decouple-3.8-py3-none-any.whl.metadata\n",
      "  Downloading python_decouple-3.8-py3-none-any.whl.metadata (14 kB)\n",
      "Downloading python_decouple-3.8-py3-none-any.whl (9.9 kB)\n",
      "Installing collected packages: python-decouple\n",
      "Successfully installed python-decouple-3.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-decouple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa2043cb-36a4-4ce6-90b3-b9abe08147f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12462308-f4de-4411-9d61-e85a6ba70640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-decouple in c:\\users\\fkhel\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages (3.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-decouple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97ab77fd-6e46-4d11-8cf0-89705e6aa893",
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefinedValueError",
     "evalue": "OPENAI_KEY not found. Declare it as envvar or define a default value.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUndefinedValueError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m openai\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOPENAI_KEY\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages\\decouple.py:248\u001b[0m, in \u001b[0;36mAutoConfig.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig:\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_caller_path())\n\u001b[1;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages\\decouple.py:107\u001b[0m, in \u001b[0;36mConfig.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    Convenient shortcut to get.\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages\\decouple.py:92\u001b[0m, in \u001b[0;36mConfig.get\u001b[1;34m(self, option, default, cast)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(default, Undefined):\n\u001b[1;32m---> 92\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UndefinedValueError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m not found. Declare it as envvar or define a default value.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(option))\n\u001b[0;32m     94\u001b[0m     value \u001b[38;5;241m=\u001b[39m default\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cast, Undefined):\n",
      "\u001b[1;31mUndefinedValueError\u001b[0m: OPENAI_KEY not found. Declare it as envvar or define a default value."
     ]
    }
   ],
   "source": [
    "openai.api_key = config('OPENAI_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dadfaf74-b89b-4524-877e-719439000e17",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'openai' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mopenai\u001b[49m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(openai\u001b[38;5;241m.\u001b[39mapi_key)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'openai' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "openai.api_key = os.getenv(\"OPENAI_KEY\")\n",
    "print(openai.api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81a63eae-b4bd-4cc3-a1ce-24a092dce3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"davinci\"  # or any other supported model\n",
    "attribute = \"happy\"\n",
    "prompt = f\"The person is {attribute}. The next word is:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42aa893c-731c-4956-bcf8-6b5d1bfaf10c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m request_result \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m#logprobs=True,\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m#max_tokens=1,\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                \u001b[49m\u001b[43mecho\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:149\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[0;32m    141\u001b[0m         timeout,\n\u001b[0;32m    142\u001b[0m         stream,\n\u001b[0;32m    143\u001b[0m         headers,\n\u001b[0;32m    144\u001b[0m         request_timeout,\n\u001b[0;32m    145\u001b[0m         typed_api_type,\n\u001b[0;32m    146\u001b[0m         requestor,\n\u001b[0;32m    147\u001b[0m         url,\n\u001b[0;32m    148\u001b[0m         params,\n\u001b[1;32m--> 149\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__prepare_create_request(\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[0;32m    153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m requestor\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:106\u001b[0m, in \u001b[0;36mEngineAPIResource.__prepare_create_request\u001b[1;34m(cls, api_key, api_base, api_type, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    104\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m MAX_TIMEOUT\n\u001b[1;32m--> 106\u001b[0m requestor \u001b[38;5;241m=\u001b[39m \u001b[43mapi_requestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAPIRequestor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43morganization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morganization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mclass_url(engine, api_type, api_version)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    115\u001b[0m     deployment_id,\n\u001b[0;32m    116\u001b[0m     engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    124\u001b[0m     params,\n\u001b[0;32m    125\u001b[0m )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages\\openai\\api_requestor.py:138\u001b[0m, in \u001b[0;36mAPIRequestor.__init__\u001b[1;34m(self, key, api_base, api_type, api_version, organization)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    131\u001b[0m     key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m     organization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    136\u001b[0m ):\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_base \u001b[38;5;241m=\u001b[39m api_base \u001b[38;5;129;01mor\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mapi_base\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m key \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_api_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_type \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    140\u001b[0m         ApiType\u001b[38;5;241m.\u001b[39mfrom_str(api_type)\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m api_type\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m ApiType\u001b[38;5;241m.\u001b[39mfrom_str(openai\u001b[38;5;241m.\u001b[39mapi_type)\n\u001b[0;32m    143\u001b[0m     )\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_version \u001b[38;5;241m=\u001b[39m api_version \u001b[38;5;129;01mor\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mapi_version\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages\\openai\\util.py:186\u001b[0m, in \u001b[0;36mdefault_api_key\u001b[1;34m()\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mapi_key\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m openai\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mAuthenticationError(\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo API key provided. You can set your API key in code using \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai.api_key = <API-KEY>\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai.api_key_path = <PATH>\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    188\u001b[0m     )\n",
      "\u001b[1;31mAuthenticationError\u001b[0m: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details."
     ]
    }
   ],
   "source": [
    "request_result = openai.ChatCompletion.create(\n",
    "                model=\"gpt4\",\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=1,\n",
    "                #logprobs=True,\n",
    "                #max_tokens=1,\n",
    "                logprobs=5,\n",
    "                echo=True\n",
    "\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072a5644-97cf-424d-94d7-2bfbfb845919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8b97cdf-98ef-4c06-85fb-e2bc771578aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fkhel\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\fkhel\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "C:\\Users\\fkhel\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted word: \n",
      "Completed sentence: fill in the blank: The people say: they be shooting  The people are \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# Load the T5 model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "\n",
    "# Input sentence with mask (T5 uses <extra_id_0> as mask token)\n",
    "#input_text = \"fill in the blank: The <extra_id_0> walked in the park, wagging its tail.\"\n",
    "input_text = \"fill in the blank: The people say: they be shooting  The people are <extra_id_0>\"\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids\n",
    "\n",
    "# Prepare decoder input (start with the pad token for T5)\n",
    "decoder_input_ids = torch.tensor([[tokenizer.pad_token_id]])\n",
    "\n",
    "# Forward pass through the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
    "    logits = outputs.logits  # Get the logits from the model output\n",
    "\n",
    "# Decode the predicted token IDs\n",
    "predicted_token_ids = torch.argmax(logits, dim=-1)  # Get the most likely token IDs\n",
    "predicted_text = tokenizer.decode(predicted_token_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Extract the predicted word (T5 predicts the mask replacement directly)\n",
    "predicted_word = predicted_text.strip()\n",
    "\n",
    "# Replace the mask in the original sentence\n",
    "completed_sentence = input_text.replace(\"<extra_id_0>\", predicted_word)\n",
    "\n",
    "print(f\"Predicted word: {predicted_word}\")\n",
    "print(f\"Completed sentence: {completed_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee0949b8-ebbe-4022-a67f-b1a69f20af0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted word: \n",
      "Completed sentence: fill in the blank: The people say: they be shooting  The people are \n"
     ]
    }
   ],
   "source": [
    "print(f\"Predicted word: {predicted_word}\")\n",
    "print(f\"Completed sentence: {completed_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca5cca62-83fe-4237-9107-cbb25c823c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_d_array = [\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb0f8807-d582-4e0f-8337-c81efbe068b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array(two_d_array)\n",
    "a.shape\n",
    "a.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa07cb4d-4d11-4673-b318-650c215fa970",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m (\u001b[43mtwo_d_array\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "(two_d_array.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01587513-b9f1-4674-a3be-ce3382d0768f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\fkhel\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install  sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c1ea11f-2479-4846-a80b-fa97178c2197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "475585d2-8b88-459c-8e55-645899f36fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protobuf\n",
      "  Obtaining dependency information for protobuf from https://files.pythonhosted.org/packages/61/fa/aae8e10512b83de633f2646506a6d835b151edf4b30d18d73afd01447253/protobuf-5.29.3-cp310-abi3-win_amd64.whl.metadata\n",
      "  Downloading protobuf-5.29.3-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Downloading protobuf-5.29.3-cp310-abi3-win_amd64.whl (434 kB)\n",
      "   ---------------------------------------- 0.0/434.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/434.5 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/434.5 kB ? eta -:--:--\n",
      "   -- ------------------------------------ 30.7/434.5 kB 325.1 kB/s eta 0:00:02\n",
      "   --- ----------------------------------- 41.0/434.5 kB 279.3 kB/s eta 0:00:02\n",
      "   ------- ------------------------------- 81.9/434.5 kB 416.7 kB/s eta 0:00:01\n",
      "   ------------ ------------------------- 143.4/434.5 kB 607.9 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 245.8/434.5 kB 885.4 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 337.9/434.5 kB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 434.5/434.5 kB 1.2 MB/s eta 0:00:00\n",
      "Installing collected packages: protobuf\n",
      "Successfully installed protobuf-5.29.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afc2097e-a3b5-4aac-b007-c27ffb44d4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "C:\\Users\\fkhel\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer for the mT5-base model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-base\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d680117a-c96e-48eb-b15e-14a6c4d2350c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: tensor([[  5448,  34994,   1093,    632,    343,    259,  17722,   4299,   1377,\n",
      "         250099,   2273,      1]])\n",
      "Decoded Text: مرحبًا، كيف حالك <extra_id_0>؟</s>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Tokenize some Arabic text\n",
    "arabic_text = \"مرحبًا، كيف حالك <extra_id_0>؟\"\n",
    "tokenized_output = tokenizer(arabic_text, return_tensors=\"pt\")\n",
    "\n",
    "print(\"Token IDs:\", tokenized_output['input_ids'])\n",
    "print(\"Decoded Text:\", tokenizer.decode(tokenized_output['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62588432-ae70-4ed0-bce4-9aa284a4847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import tqdm\n",
    "from torch.nn import functional as F\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel, \n",
    "    GPT2Tokenizer, \n",
    "    RobertaForMaskedLM, \n",
    "    RobertaTokenizer, \n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    mt5-base\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10b97e3f-0510-4ab4-974b-8efeeeef6049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fkhel\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "C:\\Users\\fkhel\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a6c772a-cd4a-4c3e-aa7b-bbf3dd8cdf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n",
      "C:\\Users\\fkhel\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('google/mt5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "778a718f-e091-48bd-812c-b3157a1ac7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "p=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9aeaf4c5-76ec-4cfd-b0db-72a4fdc9d4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "p='''<extra_id_0> \" {} \" '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bec73abb-51f8-471e-8bc2-01274e55cfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "p=p.format(\" من جد وجد ومن اجتهد \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c9c46e43-b14f-4b98-9042-37e33eaf8287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<extra_id_0> \"  من جد وجد ومن اجتهد  \" '"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e7a73f0f-df33-4069-b2f1-14d217715b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = p + \" <extra_id_0>\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5b67360e-943b-48ec-976f-3dbecdc111f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<extra_id_0> \"  من جد وجد ومن اجتهد  \"  <extra_id_0>'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d21c2faa-597c-4491-b8be-9db5cc560de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "668dfdc7-b78c-4578-8b23-79491bcb33a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If possible, move model to GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f8b07891-c042-4294-ac8a-4c2a0a95eed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<extra_id_0> \"  من جد وجد ومن اجتهد  \" '"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "42652792-c71c-46e9-9d49-dee0afcaea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([tokenizer.encode(p)])\n",
    "input_ids = input_ids.to(device)\n",
    "decoder_input_ids = torch.tensor([[tokenizer.pad_token_id]])\n",
    "decoder_input_ids = decoder_input_ids.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c6f735fb-abbb-4ef9-af55-d71fcf739196",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a6f7c012-b4c4-40ef-9efb-6370a112c80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Text: <extra_id_0> \" <extra_id_1> \" <extra_id_2> \" -\n"
     ]
    }
   ],
   "source": [
    "completed_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Completed Text:\", completed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aa388898-1139-4a2c-9f55-9b3d5af17f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids,labels=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f477e8-583d-4f88-b31f-c03b93db77ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db314d30-64f0-467f-8ba1-f1737b3a626b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Text: Paris Paris Paris Paris Paris . Paris.. Paris\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load the pre-trained T5-base model and tokenizer\n",
    "model_name = \"t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Input text with a placeholder (<extra_id_0>) for text completion\n",
    "input_text = \"The capital of France is <extra_id_0>.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate the output (completion for <extra_id_0>)\n",
    "outputs = model.generate(input_ids)\n",
    "\n",
    "# Decode the generated output\n",
    "completed_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Completed Text:\", completed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "355f6d29-3c0a-4d21-bcbc-8910d7ce2a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "sequence_ids = model.generate(input_ids)\n",
    "sequences = tokenizer.batch_decode(sequence_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97053a26-5d61-4e69-bf7f-6c01e45b4bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad> <extra_id_0> park offers <extra_id_1> the <extra_id_2> park.</s>']\n"
     ]
    }
   ],
   "source": [
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edbea95-4e3c-4920-9361-d5924d1c0f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch GPU (Python 3.10)",
   "language": "python",
   "name": "pytorch-gpu-python-3-10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
