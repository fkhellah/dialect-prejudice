{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dc4c97a-be13-4ea5-bd97-c539f43348ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fkhel\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\fkhel\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "C:\\Users\\fkhel\\miniconda3\\envs\\pytorch-gpu-python-3-10\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import tqdm\n",
    "from torch.nn import functional as F\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    RobertaForMaskedLM,\n",
    "    RobertaTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23e3171e-519b-4586-8163-7b38c44d555f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits for the next token: tensor([[-38.2558, -13.9876, -24.1940,  ..., -59.6793, -59.6874, -59.6310]],\n",
      "       device='cuda:0')\n",
      "Probabilities for the next token: tensor([[5.6643e-18, 1.9619e-07, 7.2467e-12,  ..., 2.8121e-27, 2.7894e-27,\n",
      "         2.9515e-27]], device='cuda:0')\n",
      "Predicted next token: <extra_id_0>\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Step 1: Load the pre-trained T5 model and tokenizer\n",
    "model_name = \"t5-small\"  # You can use \"t5-base\", \"t5-large\", etc.\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "model = model.to(device)\n",
    "# Step 2: Prepare the input text\n",
    "input_text = \"translate English to French: Hello\"\n",
    "input_text = \"comltete: i feel <extra_id_0>\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids  # Tokenize input\n",
    "input_ids = input_ids.to(device)\n",
    "decoder_input_ids = torch.tensor([[tokenizer.pad_token_id]])\n",
    "decoder_input_ids = decoder_input_ids.to(device)\n",
    "# Step 3: Forward pass through the model\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    outputs = model(input_ids=input_ids,decoder_input_ids=decoder_input_ids)\n",
    "\n",
    "# Step 4: Extract logits from the model's output\n",
    "logits1 = outputs.logits  # Shape: (batch_size, sequence_length, vocab_size)\n",
    "\n",
    "# Step 5: Get logits for the next token\n",
    "# The logits for the next token are located at the last position in the sequence\n",
    "next_token_logits = logits1[:, -1, :]  # Shape: (batch_size, vocab_size)\n",
    "\n",
    "# Step 6: Inspect the logits\n",
    "print(\"Logits for the next token:\", next_token_logits)\n",
    "\n",
    "# Optional: Apply softmax to convert logits to probabilities\n",
    "probs = torch.softmax(next_token_logits, dim=-1)\n",
    "print(\"Probabilities for the next token:\", probs)\n",
    "#_txt = tokenizer.decode(outputs, skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "# Step 7: Decode the predicted next token\n",
    "predicted_token_id = torch.argmax(next_token_logits, dim=-1).item()  # Get the token ID with the highest logit\n",
    "predicted_token = tokenizer.decode(predicted_token_id) \n",
    "#predicted_token = tokenizer.decode(_txt) # Decode the token ID to a string\n",
    "print(\"Predicted next token:\", predicted_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d054652-e226-4995-b06e-dcdbe580f32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits for the next token: tensor([[-18.5767,  -8.0120, -12.0802,  ..., -43.0359, -43.0601, -42.9835]],\n",
      "       device='cuda:0')\n",
      "Probabilities for the next token: tensor([2.7076e-07, 1.0490e-02, 1.7947e-04,  ..., 6.4576e-18, 6.3034e-18,\n",
      "        6.8051e-18], device='cuda:0')\n",
      "3\n",
      "Predicted next token: \n",
      "tensor([[   0,    3,   23,  473,    1,    0,    0],\n",
      "        [   0,    3,   23,  473,  114,    1,    0],\n",
      "        [   0,    3,   23,  473,    5,    1,    0],\n",
      "        [   0,    3,   23,  473,    3,   23,  473],\n",
      "        [   0,    3,   23,  473,  114,    3,   23],\n",
      "        [   0,    3,   23,  473,  473,    1,    0],\n",
      "        [   0,    3,   23,  473,  207,    5,    1],\n",
      "        [   0,    3,   23,  473,   27,  473,    1],\n",
      "        [   0,    3,   23,  473, 1245,    5,    1],\n",
      "        [   0,    3,   23,  473,  207,    1,    0]], device='cuda:0')\n",
      "Input:  i feel\n",
      "Output: i feel\n",
      "Input:  i feel\n",
      "Output: i feel like\n",
      "Input:  i feel\n",
      "Output: i feel.\n",
      "Input:  i feel\n",
      "Output: i feel feel\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load the T5 tokenizer and model\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Step 1: Load the pre-trained T5 model and tokenizer\n",
    "model_name = \"t5-small\"  # You can use \"t5-base\", \"t5-large\", etc.\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "model = model.to(device)\n",
    "# Step 2: Prepare the input text\n",
    "input_text = \" i feel\"\n",
    "input_ids = tokenizer(input_text, add_special_tokens=True, return_tensors=\"pt\").input_ids  # Tokenize input\n",
    "input_ids = input_ids.to(device)\n",
    "decoder_input_ids = torch.tensor([[tokenizer.pad_token_id]])\n",
    "decoder_input_ids = decoder_input_ids.to(device)\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    outputs = model(input_ids=input_ids,decoder_input_ids=decoder_input_ids)\n",
    "# Input text\n",
    "\n",
    "# Step 4: Extract logits from the model's output\n",
    "logits = outputs.logits  # Shape: (batch_size, sequence_length, vocab_size)\n",
    "\n",
    "# Step 5: Get logits for the next token\n",
    "# The logits for the next token are located at the last position in the sequence\n",
    "next_token_logits = logits[:, -1, :]  # Shape: (batch_size, vocab_size)\n",
    "\n",
    "# Step 6: Inspect the logits\n",
    "print(\"Logits for the next token:\", next_token_logits)\n",
    "\n",
    "# Optional: Apply softmax to convert logits to probabilities\n",
    "probs = F.softmax(outputs.logits, dim=-1)[0][-1]\n",
    "#probs = torch.softmax(next_token_logits, dim=-1)\n",
    "print(\"Probabilities for the next token:\", probs)\n",
    "\n",
    "# Step 7: Decode the predicted next token\n",
    "predicted_token_id = torch.argmax(next_token_logits, dim=-1).item()  # Get the token ID with the highest logit\n",
    "print(predicted_token_id)\n",
    "predicted_token = tokenizer.decode(predicted_token_id, skip_special_tokens=False)  # Decode the token ID to a string\n",
    "print(\"Predicted next token:\", predicted_token)\n",
    "\n",
    "\n",
    "# Generate output tokens\n",
    "output_ids = model.generate(input_ids, num_beams=200,max_length=7, num_return_sequences=10)\n",
    "print(output_ids)\n",
    "# Decode the output tokens\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Input:\", input_text)\n",
    "print(\"Output:\", output_text)\n",
    "output_text = tokenizer.decode(output_ids[1], skip_special_tokens=True)\n",
    "\n",
    "print(\"Input:\", input_text)\n",
    "print(\"Output:\", output_text)\n",
    "output_text = tokenizer.decode(output_ids[2], skip_special_tokens=True)\n",
    "\n",
    "print(\"Input:\", input_text)\n",
    "print(\"Output:\", output_text)\n",
    "\n",
    "output_text = tokenizer.decode(output_ids[5], skip_special_tokens=True)\n",
    "\n",
    "print(\"Input:\", input_text)\n",
    "print(\"Output:\", output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1db8d49d-3bea-4a37-ad2d-37f37cc2ccc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token ID: 32099\n",
      "Probabilities: tensor([[4.7018e-18, 7.5918e-07, 2.5538e-11,  ..., 3.2943e-27, 3.3372e-27,\n",
      "         3.5468e-27]], device='cuda:0')\n",
      "Predicted next word: <extra_id_0>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5Config, T5ForConditionalGeneration\n",
    "\n",
    "# Step 1: Load the pre-trained T5 model and tokenizer\n",
    "model_name = \"t5-small\"  # You can use \"t5-base\", \"t5-large\", etc.\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "#model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "#t5_config = T5Config.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "# Move the model to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "# Step 2: Prepare the input text\n",
    "input_text = \"i feel so <extra_id_0>\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=True).input_ids  # Tokenize input\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "# Step 3: Prepare decoder input IDs (start with bos token as the initial input)\n",
    "#decoder_input_ids = torch.tensor([[tokenizer.eos_token_id]]).to(device)\n",
    "#decoder_input_ids = torch.tensor([[tokenizer.bos_token_id]]).to(device)\n",
    "decoder_input_ids = torch.tensor([[tokenizer.pad_token_id]])\n",
    "decoder_input_ids = decoder_input_ids.to(device)\n",
    "# Step 4: Forward pass through the model\n",
    "model.eval()\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids,labels=None)\n",
    "\n",
    "# Step 5: Extract logits for the next token\n",
    "logits = outputs.logits  # Shape: (batch_size, sequence_length, vocab_size)\n",
    "next_token_logits = logits[:, -1, :]  # Logits for the last token in the sequence\n",
    "\n",
    "# Step 6: Convert logits to probabilities and select the most likely token\n",
    "probs = torch.softmax(next_token_logits, dim=-1)  # Apply softmax to get probabilities\n",
    "predicted_token_id = torch.argmax(probs, dim=-1).item()  # Get the token ID with the highest probability\n",
    "\n",
    "# Debugging: Print the predicted token ID and probabilities\n",
    "print(\"Predicted token ID:\", predicted_token_id)\n",
    "print(\"Probabilities:\", probs)\n",
    "\n",
    "# Step 7: Decode the predicted token ID into a word\n",
    "predicted_word = tokenizer.decode(predicted_token_id, skip_special_tokens=False)\n",
    "\n",
    "# Print the predicted next word\n",
    "print(\"Predicted next word:\", predicted_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "158fa4d0-6671-411c-828a-73ff05291240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: .\n"
     ]
    }
   ],
   "source": [
    "model_name = \"t5-base\"  # You can use \"t5-base\", \"t5-large\", etc.\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "#model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "#t5_config = T5Config.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "# Move the model to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "model = model.to(device)\n",
    "#input_text = \"i feel so <extra_id_0input_text = \"i feel so <extra_id_0>\"\n",
    "input_text = \"i feel so <extra_id_0>\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=True).input_ids  # Tokenize input\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "\n",
    "# Initialize the decoder input with the pad token\n",
    "decoder_input_ids = torch.tensor([[tokenizer.pad_token_id]]).to(device)\n",
    "\n",
    "# Generate tokens iteratively\n",
    "max_length = 10  # Maximum number of tokens to generate\n",
    "generated_tokens = []\n",
    "\n",
    "for _ in range(max_length):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
    "    logits = outputs.logits[:, -1, :]  # Logits for the last token\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    predicted_token_id = torch.argmax(probs, dim=-1).item()\n",
    "    \n",
    "    # Stop if the end-of-sequence token is generated\n",
    "    if predicted_token_id == tokenizer.eos_token_id:\n",
    "        break\n",
    "    \n",
    "    # Append the predicted token to the list\n",
    "    generated_tokens.append(predicted_token_id)\n",
    "    \n",
    "    # Update the decoder input for the next step\n",
    "    decoder_input_ids = torch.cat([decoder_input_ids, torch.tensor([[predicted_token_id]]).to(device)], dim=-1)\n",
    "\n",
    "# Decode the generated tokens into text\n",
    "generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d026354-c0cd-4451-96f3-cc8c9c12df2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ratio attribute\n",
      "0   2.50     attr1\n",
      "1   2.25     attr2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data (replace with your actual data)\n",
    "ratio_list1 = [\n",
    "    {\"aae\": 10, \"sae\": 5, \"ratio\": 2, \"variable\": \"x\", \"attribute\": \"attr1\", \"prompt\": \"p1\"},\n",
    "    {\"aae\": 20, \"sae\": 10, \"ratio\": 2, \"variable\": \"y\", \"attribute\": \"attr2\", \"prompt\": \"p2\"},\n",
    "]\n",
    "\n",
    "ratio_list2 = [\n",
    "    {\"aae1\": 15, \"sae1\": 5, \"ratio\": 3, \"variable\": \"x\", \"attribute\": \"attr1\", \"prompt\": \"p1\"},\n",
    "    {\"aae1\": 25, \"sae1\": 10, \"ratio\": 2.5, \"variable\": \"y\", \"attribute\": \"attr2\", \"prompt\": \"p2\"},\n",
    "]\n",
    "\n",
    "# Create the DataFrames\n",
    "ratio_df1 = pd.DataFrame(ratio_list1, columns=[\"aae\", \"sae\", \"ratio\", \"variable\", \"attribute\", \"prompt\"])\n",
    "ratio_df = pd.DataFrame(ratio_list2, columns=[\"aae1\", \"sae1\", \"ratio\", \"variable\", \"attribute\", \"prompt\"])\n",
    "\n",
    "# Merge the two DataFrames on common columns\n",
    "merged_df = pd.merge(ratio_df1, ratio_df, on=[\"variable\", \"attribute\", \"prompt\"], suffixes=('_df1', '_df'))\n",
    "\n",
    "# Calculate the row-wise averages of aae and aae1, and sae and sae1\n",
    "merged_df['avg_aae'] = merged_df[['aae', 'aae1']].mean(axis=1)\n",
    "merged_df['avg_sae'] = merged_df[['sae', 'sae1']].mean(axis=1)\n",
    "\n",
    "# Calculate the new ratio column\n",
    "merged_df['new_ratio'] = merged_df['avg_aae'] / merged_df['avg_sae']\n",
    "\n",
    "# Create the final DataFrame with only the desired columns\n",
    "result_df = merged_df[['new_ratio', 'attribute']].rename(columns={'new_ratio': 'ratio'})\n",
    "\n",
    "# Display the result\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae25f90-3c26-4656-8f8a-6372d26f2c73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch GPU (Python 3.10)",
   "language": "python",
   "name": "pytorch-gpu-python-3-10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
